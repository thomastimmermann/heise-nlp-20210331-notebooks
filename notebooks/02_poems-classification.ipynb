{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Klassifikation von Gedichten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Datensatz 'Deutscher Lyrik-Korpus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nutzen den Datensatz [Deutscher Lyrik-Korpus](https://github.com/thomasnikolaushaider/DLK) von Thomas Nikolaus Haider aus folgender Publikation:\n",
    "\n",
    "> Haider, T. and Eger, S. (2019, August). *Semantic Change and Emerging Tropes In a Large Corpus of New High German Poetry.* In Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change (pp. 216-222)\n",
    "\n",
    "Dieser Datensatz umfasst 60821 Deutsche Gedichte aus mehreren Jahrhunderten:\n",
    "\n",
    "![](img/poems_years.svg)\n",
    "\n",
    "Aus diesem Datensatz haben wir im Verzeichnis `data` bereits\n",
    "\n",
    "- 894 Gedichte von Heinrich Heine\n",
    "- 872 Gedichte von Johann Wolfgang Goethe und\n",
    "- 673 Gedichte von Kurt Tucholsky\n",
    "\n",
    "ausgewählt.\n",
    "\n",
    "Wir laden diese Auswahl mit [pandas](https://pandas.pydata.org) in einen [`pandas.DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "EXTRACT = 'data/poems/selected_poems.json.bz2'\n",
    "poems = pd.read_json(EXTRACT, compression='infer')\n",
    "poems.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten-Aufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Gedichte in One-Hot-kodierte Zeichenfolgen umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir bestimmen zuerst die Menge aller in den Gedichten auftretenden Zeichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "used_alphabet = set().union(*poems['text'].apply(set))\n",
    "''.join(sorted(used_alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden uns auf einen kleineren Zeichensatz beschränken und ersetzen jedes Zeichen durch\n",
    "\n",
    "- dessen Index in unserem Zeichensatz, beginnend bei 1, falls es enthalten ist, \n",
    "- eine 0, falls es nicht im Zeichensatz liegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyzäöüßABCDEFGHIKLMNOPQRSTUVWXZYÄÖÜ .,;:!?-()\"\\'\\n'\n",
    "char_index = {char: index + 1 for index, char in enumerate(ALPHABET)}\n",
    "\n",
    "def index_characters(text):\n",
    "    return [char_index.get(char, 0) for char in text]\n",
    "                                              \n",
    "poems['characters'] = poems.text.apply(index_characters)\n",
    "poems[['text', 'characters']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes ersetzen wir jeden Zeichen-Index durch den entsprechenden One-Hot-Kode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eye = np.eye(len(ALPHABET))\n",
    "zeros = np.zeros((1, len(ALPHABET)))\n",
    "codes = np.vstack([zeros, eye])\n",
    "codes.shape, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "poems['characters_ohe'] = poems.characters.apply(lambda chars: codes[chars])\n",
    "poems['characters_ohe'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Schluss packen wir die Folgen der One-Hot-Kodes in eine Matrix und schneiden dabei die Gedichte auf eine feste Länge. Dazu verwenden wir die Hilfsfunktion [`pad_sequences`](https://keras.io/preprocessing/sequence/) von [Keras](https://keras.io):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 1000\n",
    "X = pad_sequences(poems.characters_ohe, maxlen=MAX_LEN)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: Autoren kodieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes kodieren wir die Label, also die Autoren. Das können wir ähnlich wie oben machen oder mit Hilfe der Funktion [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) von [Pandas](https://pandas.pydata.org):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "authors_ohe = pd.get_dummies(poems.author)\n",
    "authors_ohe.head()\n",
    "authors = authors_ohe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit erhalten wir unsere Label wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y = authors_ohe.values\n",
    "y.shape, y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3: Aufteilung in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes zerlegen wir unsere Daten in eine Trainings- und eine Test-Menge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  train_size=0.7)\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifikation mit einem neuronalen Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 1: einfaches Feed-Forward-Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probieren wir die Klassifikation mit einem einfachen Netz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "def build_model():\n",
    "    return Sequential([\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def train_model(model, epochs=5, batch_size=32):\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adam')\n",
    "    history = model.fit(X_train,y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "    return model, pd.DataFrame(history.history)\n",
    "    \n",
    "model = build_model()\n",
    "model, history = train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beobachten ein massives Over-Fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 2: Faltungsschichten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes trainieren wir ein Faltungsnetz mit\n",
    "\n",
    "- mehreren [**Faltungsschichten**](https://keras.io/layers/convolutional/) zur Muster-Extraktion und\n",
    "- einer [**dichten Schicht**](https://keras.io/layers/core/) zur Klassifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "def build_model():\n",
    "    return Sequential([\n",
    "        Conv1D(64, kernel_size=3, strides=1, activation='relu', input_shape=(MAX_LEN, len(ALPHABET))),\n",
    "        Conv1D(128, kernel_size=3, strides=1, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model, history = train_model(build_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisieren wir den Trainingsverlauf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "def plot_history(history):\n",
    "    _, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "    history[['loss', 'val_loss']].plot.line(ax=ax1)\n",
    "    history[['accuracy', 'val_accuracy']].plot.line(ax=ax2)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Werten wir schließlich das Modell auf den Testdaten aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    return y_true, y_pred\n",
    "\n",
    "y_true, y_pred = validate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun schauen wir uns die Konfusionsmatrix und ein paar Scores an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion(y_true, y_pred):\n",
    "    confusion_matrix = pd.crosstab(y_pred, y_true)\n",
    "    confusion_matrix.index = pd.Index(authors, name=\"Vorhersage\")\n",
    "    confusion_matrix.columns = pd.Index(authors, name=\"Wahrheit\")\n",
    "    return confusion_matrix\n",
    "\n",
    "confusion(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist nicht überraschend, dass Goethe und Heine eher miteinander verwechselt werden als mit Tucholsky..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe: Trainieren einer Einbettungs-Schicht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können das Modell auch mit Zeichen-Index-Folgen statt mit Zeichen-One-Hot-Code-Folgen füttern und die [`Embedding`](https://keras.io/layers/embeddings/)-Schicht von Keras verwenden, um im Modell eine Zeichen-Einbettung zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(poems.characters, maxlen=MAX_LEN)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwende nun als erste Schicht [`Embedding`](https://keras.io/layers/embeddings/) und füttere das Modell mit Zeichen-Index-Folgen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, MaxPooling1D\n",
    "\n",
    "def build_embedding_model():\n",
    "# Your code here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schau, ob das Modell das tut, was es soll!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model, _ = train_model(build_embedding_model(), epochs=10)\n",
    "confusion(*validate(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "name": "02-character-level.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
