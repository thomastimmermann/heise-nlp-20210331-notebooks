{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wort-Einbettungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wort-Einbettungen sind große Lookup-Tabellen, die jedem Wort aus einem festen Vokabular (je größer, desto besser) einen hoch-dimensionalen Vektor zuordnen (typisch sind Dimensionen zwischen 70 und 300), sodass ähnliche Worte auf ähnliche Vektoren abgebildet werden.\n",
    "\n",
    "Zur Veranschaulichung spielen wir ein bisschen mit den Wort-Einbettungen, die von der NLP-Bibliothek [spaCy](https://spacy.io) im deutschen Sprachmodell bereitgestellt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy-Sprachmodell mit Wort-Einbettungen laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dazu müssen wir erst per Kommando-Zeile das deutsche Sprachmodell mit spaCy aus dem Netz herunterladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy bietet drei deutsche Sprachmodelle verschiedener Größe an: klein, mittel und groß.\n",
    "Um die Wort-Einbettungen verwenden zu können, laden wir das Sprachmodell in Python wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "vocab = nlp.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erhalten wir zu jedem Wort des Vokabulars wie folgt den Wort-Vektor des Sprachmodells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vocab.get_vector('Hund')\n",
    "vector.shape, vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wort-Vektoren haben hier also die Dimension 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ähnlichkeit von Wort-Vektoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen Wort-Vektoren können wir bereits dem Computer Spiele wie \"Find the odd one out\" beibringen. Dabei ist die Aufgabe, aus einer Reihe von Wörtern das Wort herauszufinden, das am wenigsten zu den anderen passt.\n",
    "\n",
    "Als ersten Schritt berechnen wir zu einer Liste von Wörtern $w_1, \\ldots, w_n$ die Ähnlichkeitsmatrix der zugehörigen Wortvektoren $v_1, \\ldots, v_n$. Der Eintrag der Matrix an der Stelle $i,j$ ist das Skalarprodukt von $v_i$ und $v_j$ geteilt durch die Normen der Vektoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def similarity(words):\n",
    "    vectors = np.asarray(list(map(vocab.get_vector, words)))\n",
    "    norms = np.linalg.norm(vectors, axis=-1)\n",
    "    normed_vectors = vectors / np.reshape(norms, (-1,1))\n",
    "    matrix = np.matmul(normed_vectors, normed_vectors.transpose())\n",
    "    return pd.DataFrame(matrix, columns=words, index=words)\n",
    "\n",
    "words = ['Hund', 'Katze', 'Schuh']\n",
    "similarity(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Veranschaulichung plotten wir eine Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "words = ['Hund', 'Katze', 'Schuh', 'Wal', 'Fisch']\n",
    "sns.heatmap(similarity(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiel \"Find the odd one out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun implementieren wir das oben genannte Spiel und suchen aus einer Liste von Wörtern das heraus, das den anderen am unähnlichsten ist.\n",
    "Dazu summieren wir die Ähnlichkeiten in der Ähnlichkeitsmatrix entlang jeder Zeile und wählen das Wort mit der niedrigsten Summe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def find_the_odd_one(words):\n",
    "    similarity_matrix = similarity(words).values\n",
    "    similarity_sums = np.sum(similarity_matrix, axis=-1)\n",
    "    return words[np.argmin(similarity_sums)]\n",
    "\n",
    "                       \n",
    "find_the_odd_one(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das funktioniert ganz gut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_the_odd_one(['Computer', 'Technik', 'Informatik', 'Kochen', 'Programmieren'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projektionen von Wort-Einbettungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wort-Einbettungen bewahren (idealerweise) nicht nur die Ähnlichkeit von Wörtern. Das kann man durch zwei-dimensionale Projektionen der Wort-Vektoren veranschaulichen, die man durch klassische Dimensionsreduktions-Verfahren wie PCA erhält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "def pca(words):\n",
    "    vectors = np.array(list(map(vocab.get_vector, words)))\n",
    "    vectors_2d = decomposition.PCA(2).fit_transform(vectors)\n",
    "    return pd.DataFrame(vectors_2d, columns=['x', 'y'], index=words)\n",
    "    \n",
    "pca(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def project(words):\n",
    "    df = pca(words).reset_index().rename(columns={'index':'Wort'})\n",
    "    base = alt.Chart(df).encode(x='x', y='y')\n",
    "    return base.mark_point() + base.encode(text='Wort').mark_text(dy=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project(['Paris', 'Frankreich', 'London', 'Großbritannien', 'Berlin', 'Deutschland'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
